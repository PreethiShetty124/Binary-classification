#Importing Necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import os
import zipfile
import keras
import tensorflow as tf
from tensorflow import keras
from keras.preprocessing import image
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from google.colab import files

# loading the data
local_zip = "/content/drive/MyDrive/Colab Notebooks/Horse and human images/horse-or-human.zip"
zip_ref = zipfile.ZipFile(local_zip, "r")
zip_ref.extractall('/content/drive/MyDrive/Colab Notebooks/Horse and human images')
zip_ref.close()

# Directory with our training horse pictures
train_horse_dir = os.path.join('/content/drive/MyDrive/Colab Notebooks/Horse and human images/horse-or-human/train/horses')
 
# Directory with our training human pictures
train_human_dir = os.path.join('/content/drive/MyDrive/Colab Notebooks/Horse and human images/horse-or-human/train/humans')

# Directory with our validation horse pictures
val_horse_dir = os.path.join('/content/drive/MyDrive/Colab Notebooks/Horse and human images/horse-or-human/validation/horses')
 
# Directory with our validation human pictures
val_human_dir = os.path.join('/content/drive/MyDrive/Colab Notebooks/Horse and human images/horse-or-human/validation/humans')

# To see the filenames of horse and humans in the directories

# Training Directories
train_horse_names = os.listdir(train_horse_dir)
print(train_horse_names[:5])
train_human_names = os.listdir(train_human_dir)
print(train_human_names[:5])

#Validation Directories
validation_horse_names = os.listdir(val_horse_dir)
print(validation_horse_names[:5])
validation_human_names = os.listdir(val_human_dir)
print(validation_human_names[:5])

# to check the total number of images present in training and validation directories
print('total training horse images:', len(os.listdir(train_horse_dir)))
print('total training human images:', len(os.listdir(train_human_dir)))
print('total validation horse images:', len(os.listdir(val_horse_dir)))
print('total validation human images:', len(os.listdir(val_human_dir)))

# Data Visualization
# Parameters for our graph
nrows = 4
ncols = 4
# Index for iterating over images 
pic_index = 0
# Setup matplotlib figure
fig = plt.gcf()
fig.set_size_inches(ncols*4, nrows*4)

pic_index += 8
next_horse_px = [os.path.join(train_horse_dir, fname) for fname in train_horse_names[pic_index-8:pic_index]]
next_human_px = [os.path.join(train_human_dir, fname) for fname in train_human_names[pic_index-8:pic_index]]

for i, img_path in enumerate(next_horse_px+next_human_px):
  # Set subplots
  sp = plt.subplot(nrows, ncols, i+1)
  sp.axis("Off")
  img = mpimg.imread(img_path)
  plt.imshow(img)
plt.show()

# Building Convolutional Neural Network from scratch
model = tf.keras.models.Sequential([
                                    #First convolution
                                    tf.keras.layers.Conv2D(16, (3, 3), activation="relu", input_shape=(300, 300, 3)),
                                    tf.keras.layers.MaxPooling2D(2, 2),
                                    # The second convolution
                                    tf.keras.layers.Conv2D(32, (3, 3), activation="relu"),
                                    tf.keras.layers.MaxPooling2D(2, 2),
                                    # The third convolution
                                    tf.keras.layers.Conv2D(64, (3, 3), activation="relu"),
                                    tf.keras.layers.MaxPooling2D(2, 2),
                                    # The fourth convolution
                                    tf.keras.layers.Conv2D(64, (3, 3), activation="relu"),
                                    tf.keras.layers.MaxPooling2D(2, 2),
                                    # The fifth convolution
                                    tf.keras.layers.Conv2D(64, (3, 3), activation="relu"),
                                    tf.keras.layers.MaxPooling2D(2, 2),
                                    # Flatten the results to feed in Deep Neural Network
                                    tf.keras.layers.Flatten(),
                                    tf.keras.layers.Dense(512, activation="relu"), #512 neuron hidden layer
                                    tf.keras.layers.Dense(1, activation="sigmoid")# only 1 output neuron   
                                    ])
                                   
# Summary of Neural Network
model.summary() 

# Compile the Model
model.compile(loss="binary_crossentropy",optimizer='adam',metrics=["accuracy"])

# Normalizing all the images
# All images are rescaled by 1./255
train_datagen = ImageDataGenerator(rescale=1./255)
validation_datagen = ImageDataGenerator(rescale=1./255)

# Flow training images in batches of 128 using train_datagen generator
train_generator = train_datagen.flow_from_directory(
    "/content/drive/MyDrive/Colab Notebooks/Horse and human images/horse-or-human/train",
    target_size=(300, 300),
    batch_size=128,
    class_mode="binary"
)

# Flow validation images in batches of 32 using validation_datagen generator
validation_generator = validation_datagen.flow_from_directory(
    "/content/drive/MyDrive/Colab Notebooks/Horse and human images/horse-or-human/validation",
    target_size=(300, 300),
    batch_size=32, 
    class_mode="binary"
)

# Training the Model
history = model.fit(
    train_generator,
    steps_per_epoch=8, 
    epochs=15,
    verbose=1,
    validation_data=validation_generator,
    validation_steps=8
)

import numpy as np
from google.colab import files
import keras.utils as image
 
uploaded = files.upload()
 
for fn in uploaded.keys():
 
  # predicting images
  path = '/content/' + fn
  img = image.load_img(path, target_size=(300, 300))
  x = image.img_to_array(img)
  x = np.expand_dims(x, axis=0)
 
  images = np.vstack([x])
  classes = model.predict(images, batch_size=10)
  print(classes[0])
  if classes[0]>0.5:
    print(fn + " is a human")
  else:
    print(fn + " is a horse")
                                   
